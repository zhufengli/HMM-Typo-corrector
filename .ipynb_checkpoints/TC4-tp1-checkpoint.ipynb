{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zT0dXdRMf_LG"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Remarque préalable\n",
    "\n",
    "Les TP sont en python. Il est important d'en garder la trace convenablement: soit sous la forme d'un script ou d'un notebook. Néanmoins il faut réfléchir aux structures de données à mettre en oeuvre et penser à créer des fonctions, facilement ré-utilisables et bien paramètrées. \n",
    "\n",
    "Les toolkits utiles sont en général installés dans: \n",
    " /partage/public/allauzen/python2.7/site-packages\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# python et ipython notebook\n",
    "\n",
    "Les TPs seront fait en python et avec les notebook ipython. Donc pour bien commencer, regarder les tutoriels suivant: \n",
    "* Pour python et numpy: http://cs231n.github.io/python-numpy-tutorial/\n",
    "* Pour ipython : http://cs231n.github.io/ipython-tutorial/\n",
    "\n",
    "Pour bien faire ce TP, le plus simple est d'utiliser sa version notebook, soit le fichier de départ \n",
    " TC4-tp1.ipynb\n",
    " \n",
    "Pour lancer une session, créer un répertoire puis y copier le fichier *ipynb*. Placer vous dans ce répertoire executer:\n",
    " jupytrt notebook\n",
    " \n",
    "Sélectionner le fichier que vous souhaiter à savoir *TC4-tp1.ipynb*. C'est parti.\n",
    "\n",
    "\n",
    "\n",
    "# Les données : le Brown corpus\n",
    "\n",
    "L'objectif de cette première partie est de charger les données et de regarder (statistiquement) ce qu'elles contiennent. Le corpus utilisé contient du texte étiqueté en partie du discours, ou *POS tags* pour *Part Of Speech*: à chaque mot d'une phrase est associé une classe grammaticale. Ainsi une séquence de mot doit être associée à une séquence de tags (de même longueur).   Le corpus est organisé comme un ensemble de phrases. À chaque mot est associé une catégorie grammaticale et donc chaque phrase est une séquence de mots à laquelle est associée une séquence de catégorie. \n",
    "\n",
    "Supposons qu'un mot est la réalisation d'une variable aléatoire notée *X* et que son étiquette est la réalisation de la variable aléatoire *Y*. \n",
    "\n",
    "\n",
    "Pour le chargé, il y a deux possibilités. \n",
    "\n",
    "##  Solution 1:  NLTK\n",
    "Les données sont distribuées avec le toolkit *NLTK (Natural Language ToolKit)* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FHtZPQ8gJZ5l"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1473
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 620,
     "status": "error",
     "timestamp": 1539681381541,
     "user": {
      "displayName": "Xihui Wang",
      "photoUrl": "",
      "userId": "09445164112052208872"
     },
     "user_tz": -120
    },
    "id": "mLdK0SLNf_LI",
    "outputId": "4c7f4534-0c3c-4c52-cf52-3b5859caf7a3"
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLy4O0cSf_LS"
   },
   "source": [
    "## Solution 2: Pickle\n",
    "Si vous faites des traitements sur les données, il est toujours possible de sauvegarder sur disque la version traitée pour ne pas avoir à recommencer depuis le début. En python, la serialisation d'objet se nomme pickle et est très simple d'utilisation. Dans notre cas, data contient plus que les données. Nous allons ici simplement extraire les données dans une liste pour pouvoir les sauver. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73LnFFzcf_LV"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = pickle.load(open( \"brown.save.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9rKmuOKf_La"
   },
   "source": [
    "Pour charger les données à partir du fichier ainsi créé, il suffit de :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1154,
     "status": "ok",
     "timestamp": 1539637190433,
     "user": {
      "displayName": "Xihui Wang",
      "photoUrl": "",
      "userId": "09445164112052208872"
     },
     "user_tz": -120
    },
    "id": "6HR2C4uPf_Le",
    "outputId": "93333cdf-d113-4a14-e75b-a1c3b1f09002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(data[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTNVGdwpf_Lh"
   },
   "source": [
    "Voici donc la première phrase du corpus, vue comme une séquence de couple *(mot,label)*. La première chose à faire est de diviser les données en 3 parties: apprentissage (*train*), développement (*dev*) et évaluation (*test*). Une répartition est en gros 80%, 10%, 10% respectivement. \n",
    "\n",
    "## S'échauffer en python\n",
    "\n",
    "* De quel type est la variable data ? Et pour data[0] ? \n",
    "* Combien y-a-t-il de phrases dans le corpus ? \n",
    "* Combien y-a-t-il de mots (nombre d'occurence, par exemple il y a 25 mots dans la première phrase) ? \n",
    "* Quel est la taille du vocabulaire (la liste des mots qui apparaissent au moins une fois dans le corpus) ? Pour répondre à cette question, une solution est de construire un set en parcourant le corpus. \n",
    "* Quel est la liste des tags utilisés ? Combien y-en-t-il ? \n",
    "* Construire une map qui stocke l'association (\"mot\",compte du mot) et estimer ces comptes sur tous le corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 617,
     "status": "error",
     "timestamp": 1539681466176,
     "user": {
      "displayName": "Xihui Wang",
      "photoUrl": "",
      "userId": "09445164112052208872"
     },
     "user_tz": -120
    },
    "id": "OUWlaU-nf_Li",
    "outputId": "9ed44a04-79fa-40c1-e4bf-1dfb0c41e7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "Number of phrase: 57340\n",
      "Number of word: 1161192\n",
      "Number of vocabulaire: 56057\n",
      "['DET', 'NOUN', 'ADJ', 'VERB', 'ADP', '.', 'ADV', 'CONJ', 'PRT', 'PRON', 'NUM', 'X']\n"
     ]
    }
   ],
   "source": [
    "#type de data\n",
    "print(type(data))\n",
    "\n",
    "#type de data[0]\n",
    "print(type(data[0]))\n",
    "\n",
    "#nb de phrases\n",
    "print('Number of phrase: {}'.format(len(data)))\n",
    "\n",
    "#nb de mots\n",
    "nbword=0\n",
    "for i in range(len(data)):\n",
    "    nbword+=len(data[i])\n",
    "print('Number of word: {}'.format(nbword))\n",
    "\n",
    "#Nb de vocabulaire\n",
    "vocabulaire=[]\n",
    "for phrase in range(len(data)):\n",
    "    for mot in range(len(data[phrase])):\n",
    "        if data[phrase][mot][0] not in vocabulaire:\n",
    "            vocabulaire.append(data[phrase][mot][0])\n",
    "print ('Number of vocabulaire: {}'.format(len(vocabulaire)))\n",
    "\n",
    "\n",
    "tags=[]\n",
    "for phrase in range(len(data)):\n",
    "    for mot in range(len(data[phrase])):\n",
    "        if data[phrase][mot][1] not in tags:\n",
    "            tags.append(data[phrase][mot][1])\n",
    "print (tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cBn5QKDJf_Lk"
   },
   "source": [
    "## Questions \n",
    "\n",
    "1. Comment faire une séparation aléatoire des données qui respectent (à peu près) la répartition proposée ? \n",
    "2. Réaliser ce partage. \n",
    "3. Sur les données d'apprentissage, contruire l'espace de réalisation concernant les étiquettes, calculer la distribution et la représenter avec matplotlib. Estimer et représenter la distribution mesurée sur les données de *dev* également. \n",
    "4. Pour les mots, construire également l'espace de réalisation et calculer le compte de chaque mot. Puis représenter les comptes de comptes (soit combien de mots apparaissent une fois, deux fois, ... ). \n",
    "5. Représenter l'histogramme correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3ESvRevmd3B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5733\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def map_function(data, classe):\n",
    "    association={}\n",
    "    if classe=='vocabulaire':\n",
    "        index=0\n",
    "    else:\n",
    "        index=1\n",
    "    for phrase in range(len(data)):\n",
    "        for mot in range(len(data[phrase])):\n",
    "            if data[phrase][mot][index] not in association.keys():\n",
    "                association[data[phrase][mot][index]] = 1\n",
    "            else:\n",
    "                association[data[phrase][mot][index]] += 1\n",
    "    return association\n",
    "\n",
    "def nb_mots(mot_dict):\n",
    "    nb=sorted(mot_dict.values())\n",
    "    table_compte=np.zeros(nb[-1])\n",
    "    i=1\n",
    "    for j in range(len(nb)):\n",
    "        if nb[j]==i:\n",
    "            table_compte[i-1]=table_compte[i-1]+1\n",
    "        else:\n",
    "            i=nb[j]\n",
    "            table_compte[i-1]=table_compte[i-1]+1\n",
    "    return table_compte\n",
    "\n",
    "def create_set(data, train_per, dev_per):\n",
    "    #separer des données suivant leur pourcentage\n",
    "    train_set=[]\n",
    "    dev_set=[]\n",
    "    test_set=[]\n",
    "    num=int(train_per*len(data))\n",
    "    train_set=data[0:num]\n",
    "    num2=int(dev_per*len(data))\n",
    "    dev_set=data[num+1:(num+num2)]\n",
    "    test_set=data[(num+num2)+1:len(data)]\n",
    "    return train_set, dev_set, test_set\n",
    "\n",
    "train_set, dev_set, test_set=create_set(data, 0.8, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquette_train=map_function(train_set,'tags')\n",
    "print('equiquette compte sur le train_set : {}'.format(etiquette_train))\n",
    "\n",
    "mots_train=map_function(train_set, 'vocabulaire')\n",
    "table_compte=nb_mots(mots_train)\n",
    "print ('compte mot : {}'.format(table_compte))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(etiquette_train.keys(), etiquette_train.values())\n",
    "\n",
    "etiquette_dev=map_function(dev_set,'tags')\n",
    "print ('equiquette compte sur le dev_set : {}'.format(etiquette_dev))\n",
    "plt.figure()\n",
    "plt.bar(etiquette_dev.keys(), etiquette_dev.values())\n",
    "\n",
    "#ici y un bug.....\n",
    "plt.bar(range(1,len(table_compte)+1), table_compte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qYb6QJgZf_Ll"
   },
   "source": [
    "# Probabilités\n",
    "\n",
    "\n",
    "1. Nous allons limiter le vocabulaire aux mots apparaissant 10 fois ou plus. Les autres mots sont tous remplacés par la même forme \\*unk\\*\n",
    "2. Estimer la distribution *P(Y|X)* sur les données d'apprentissage\n",
    "3. Sur les données de test, effectuer la prédiction des etiquettes les plus probables et comparer votre prédiction aux etiquettes de référence et calculer le taux d'erreur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rsgBFrgcf_Lm",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6430\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#garde que des mots qui ont une occurence plus ou égale à 10\n",
    "mots_train['*unk*']=10\n",
    "table=[]\n",
    "\n",
    "for key in mots_train.keys():\n",
    "    if mots_train[key]<10:\n",
    "        mots_train['*unk*']=mots_train['*unk*']+mots_train[key]\n",
    "        table.append(key)\n",
    "mots_train['*unk*']=mots_train['*unk*']-10\n",
    "for mot in table:\n",
    "    del mots_train[mot]\n",
    "\n",
    "#estimation de P(Y|X)\n",
    "#il suffit de compter pour chaque étiquette, le nombre d'occurence de l'étiquette sur le nombre total d'occurence de X\n",
    "#On definit dans la suite une nouvelle map function, qui crée un dictionaire de dictionaire.\n",
    "def new_map_function(data):\n",
    "    association={}\n",
    "    for phrase in range(len(data)):\n",
    "        for mot in range(len(data[phrase])):\n",
    "            if data[phrase][mot][0] not in association.keys():\n",
    "                association[data[phrase][mot][0]] = {'DET':0, 'NOUN':0, 'ADJ':0, 'VERB':0, 'ADP':0, '.':0, 'ADV':0, 'CONJ':0, 'PRT':0, 'PRON':0, 'NUM':0, 'X':0}\n",
    "            association[data[phrase][mot][0]][data[phrase][mot][1]]+=1\n",
    "    return association\n",
    "\n",
    "data_dict=new_map_function(data)\n",
    "print (train_dict['The']['DET'])\n",
    "\n",
    "#on crée une dictionaire\n",
    "P={}\n",
    "for x in data_dict.keys():\n",
    "    count=0\n",
    "    for y in tags:\n",
    "        count+=data_dict[x][y]\n",
    "    for y in tags:\n",
    "        P[(x,y)]=data_dict[x][y]/count\n",
    "        \n",
    "print (P[('The','DET')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taux d'erreur: 0.04728873054958872\n"
     ]
    }
   ],
   "source": [
    "#prédiction sur test set\n",
    "count=0.0\n",
    "total=0.0\n",
    "for phrase in range(len(test_set)):\n",
    "    for mot in range(len(test_set[phrase])):\n",
    "        p=0\n",
    "        prediction=None\n",
    "        for tag in tags:\n",
    "            if P[(test_set[phrase][mot][0],tag)]>p:\n",
    "                p=P[(test_set[phrase][mot][0],tag)]\n",
    "                prediction=tag\n",
    "        if prediction!=test_set[phrase][mot][1]:\n",
    "            count+=1\n",
    "        total+=1\n",
    "print (\"taux d'erreur: {}\".format(count/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TC4-tp1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
