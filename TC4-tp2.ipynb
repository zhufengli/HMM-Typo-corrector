{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwILkW1F8FnJ"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Ce TP continue le TP précédent. Nous allons reprendre d'ailleurs les mêmes données et commencer la mise en oeuvre d'un modèle de Markov pour la prédiction des étiquettes: \n",
    "* une observation est une phrase, représentée comme une séquence de variables aléatoires, une par mot de la phrase\n",
    "* à cette observation est associée une séquence de variables aléatoires représentant les étiquettes, une par mot de la phrase également\n",
    "\n",
    "On suppose que la séquence d'observation (une phrase) est générée par un modèle de Markov caché. Les variables cachées sont donc les étiquettes à inférer. Nous allons commencer par écrire une classe python pour représenter le HMM. Cette classe évoluera au fil des TPs. \n",
    "\n",
    "Pour cela le code de départ suivant est donné. Afin d'initialiser un HMM, nous devons connaitre : \n",
    "- l'ensemble des états (ou *state_list*), dans notre cas l'ensemble des étiquettes grammaticales;\n",
    "- l'ensemble des observations (ou *observation_list*), dans notre cas l'ensemble des mots connus; tous les autres mots seront remplacés par l'élément spécial *UNK* qui fait partie de l'ensemble des observations. \n",
    "\n",
    "Enfin, en interne il est plus facile d'indexer les mots et et les états par des entiers. Ainsi à chaque éléments de respectivement l'ensemble des états et l'ensemble des observations, est associé un indice. Cela nous permet de tout traiter en \"matricielle\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtP9d0Pz8FnL"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros\n",
    "from __future__ import division\n",
    "import sys\n",
    "\n",
    "# Some words in test could be unseen during training, or out of the vocabulary (OOV) even during the training. \n",
    "# To manage OOVs, all words out the vocabulary are mapped on a special token: UNK defined as follows: \n",
    "UNK = \"<unk>\" \n",
    "UNKid = 0 \n",
    "\n",
    "class HMM:\n",
    "        def __init__(self, state_list, observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None):\n",
    "            \"\"\"Builds a new Hidden Markov Model\n",
    "            state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "            print \"HMM creating with: \"\n",
    "            self.N = len(state_list) # The number of states\n",
    "            self.M = len(observation_list) # The number of words in the vocabulary\n",
    "            print str(self.N)+\" states\"\n",
    "            print str(self.M)+\" observations\"\n",
    "            self.omega_Y = state_list # Keep the vocabulary of tags\n",
    "            self.omega_X = observation_list # Keep the vocabulary of tags\n",
    "            # Init. of the 3 distributions : observation, transition and initial states\n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.M, self.N), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "            if initial_state_proba is None:\n",
    "                self.initial_state_proba = zeros( (self.N,), float ) \n",
    "            else:\n",
    "                self.initial_state_proba=initial_state_proba\n",
    "            # Since everything will be stored in numpy arrays, it is more convenient and compact to \n",
    "            # handle words and tags as indices (integer) for a direct access. However, we also need \n",
    "            # to keep the mapping between strings (word or tag) and indices. \n",
    "            self.make_indexes()\n",
    "\n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities arrays\"\"\"\n",
    "            self.Y_index = {}\n",
    "            for i in range(self.N):\n",
    "                self.Y_index[self.omega_Y[i]] = i\n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[self.omega_X[i]] = i\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuO2bOCE8FnR"
   },
   "source": [
    "# Interface avec les données et apprentissage supervisé\n",
    "\n",
    "Ainsi pour initialiser un HMM, nous allons devoir lire les données (chose faîte lors du TP précédent): \n",
    "* écrire une fonction permettant d'initialiser le HMM à partir des données d'apprentissage\n",
    "* écrire une fonction *apprentissage_supervisé* qui permet d'estimer les paramètres \n",
    "\n",
    "Dans un premier temps, nous limiterons, comme lors du TP précédent, le vocabulaire aux mots apparaissant 10 fois ou plus. Les autres mots sont tous remplacés par la même forme *unk*\n",
    "\n",
    "Pour cela, le plan de travail peut être envisagé ainsi: \n",
    "* Lire les données puis générer un corpus de **train** (80%) puis de **test** (10%)\n",
    "* écrire une fonction qui créer à partir des données d'apprentissage (**train**), tous les comptes nécessaires pour l'estimation supervisée des paramètres du HMM\n",
    "* écrire 3 fonctions qui estimes les paramètres à partir des comptes, une fonction par distribution: observation, transition, état initial. \n",
    "* écrire une fonction qui reprend le tout et qui estime tous les paramètres du HMM\n",
    "\n",
    "\n",
    "# Exercice : Algorithme de Viterbi\n",
    "\n",
    "La question qui se pose est comment calculer la meilleure séquence d'étiquettes pour une phrase donnée connaissant les paramètres du HMM. Par meilleure, on entend la séquence d'étiquettes (ou d'états) la plus probable connaissant la séquence d'obervation. \n",
    "\n",
    "Proposer et implémenter un algorithme répondant à cette question. Pour vous aider à démarrer, cet algorithme s'appelle Viterbi et regardez cette vidéo https://www.youtube.com/watch?v=RwwfUICZLsA, pour comprendre comment il opère. \n",
    "\n",
    "# TODO pour la prochaine fois\n",
    "\n",
    "* Finir la partie interface (qui comprend l'apprentissage supervisé)\n",
    "* Regarder la vidéo et implémenter l'algorithme de Viterbi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YO0E0GvD8FnS",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef pr\\xc3\\xa9processe(data)\\n    for phrase in range(len(data)):\\n        for mot in range(len(data[phrase])):\\n            if data[phrase][mot][0] not in dictionary:\\n                data[phrase][mot][0]=\"<unk>\"\\n                data[phrase][mot][1]=\"0\"\\n    return data\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lire des données\n",
    "import numpy as np\n",
    "import pickle\n",
    "data = pickle.load(open( \"brown.save.p\", \"rb\" ))\n",
    "\n",
    "#créer un dictionaire pour les étiquettes, dans la suite on va utiliser les listes au lieu de dictionaire\n",
    "\n",
    "#state_list\n",
    "etiquette={'0':0,'DET':1, 'NOUN':2, 'ADJ':3, 'VERB':4, 'ADP':5, '.':6, 'ADV':7, 'CONJ':8, 'PRT':9, 'PRON':10, 'NUM':11, 'X':12}\n",
    "\n",
    "#Générer training set et test set\n",
    "def create_set(data, train_per, dev_per):\n",
    "    #separer des données suivant leur pourcentage\n",
    "    train_set=[]\n",
    "    dev_set=[]\n",
    "    test_set=[]\n",
    "    num=int(train_per*len(data))\n",
    "    train_set=data[0:num]\n",
    "    num2=int(dev_per*len(data))\n",
    "    dev_set=data[num+1:(num+num2)]\n",
    "    test_set=data[(num+num2)+1:len(data)]\n",
    "    return train_set, dev_set, test_set\n",
    "\n",
    "def map_function(data, etiquette):\n",
    "    #a map function\n",
    "    association={}\n",
    "    for phrase in range(len(data)):\n",
    "        for mot in range(len(data[phrase])):\n",
    "            if data[phrase][mot][0] not in association.keys():\n",
    "                association[data[phrase][mot][0]] = np.zeros(13)\n",
    "            association[data[phrase][mot][0]][etiquette[data[phrase][mot][1]]]+=1\n",
    "            \n",
    "    association[\"<unk>\"]=np.zeros(13)\n",
    "    unk_list=[]\n",
    "    \n",
    "    #observation_list\n",
    "    dictionary=[]\n",
    "    \n",
    "    for mot in association.keys():\n",
    "        occurence=association[mot].sum()\n",
    "        if occurence<10:\n",
    "            unk_list.append(mot)\n",
    "            association[\"<unk>\"][0]+=occurence\n",
    "        else:\n",
    "            dictionary.append(mot)\n",
    "            \n",
    "    for mot in unk_list:\n",
    "        del association[mot]\n",
    "        \n",
    "    return association, dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def préprocesse(data)\n",
    "    for phrase in range(len(data)):\n",
    "        for mot in range(len(data[phrase])):\n",
    "            if data[phrase][mot][0] not in dictionary:\n",
    "                data[phrase][mot][0]=\"<unk>\"\n",
    "                data[phrase][mot][1]=\"0\"\n",
    "    return data\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107008.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.]\n",
      "124\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set=create_set(data, 80, 10)\n",
    "\n",
    "map_train, dictionary = map_function(train_set, etiquette)\n",
    "\n",
    "print(map_train[\"<unk>\"])\n",
    "\n",
    "print (dictionary.index(\"<unk>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dans la suite il suffit de calculer les paramètres transition_proba, observation_proba, initial_state_proba\n",
    "def calculate_transition_proba(data):\n",
    "    #compter chaque transition, chaque colonne diviser par la somme colonne.\n",
    "    tags=[]\n",
    "    for phrase in range(len(data)):\n",
    "        for mot in range(len(data[phrase])):\n",
    "            if data[phrase][mot][1] not in tags:\n",
    "                tags.append(data[phrase][mot][1])\n",
    "    tags_dic={}\n",
    "    for i,j in zip(tags,range(0,len(tags))):\n",
    "        tags_dic[i]=j\n",
    "    #Transition matrix\n",
    "    transition = np.zeros([len(tags),len(tags)])\n",
    "    for phrase in range(len(data)):\n",
    "        for mot in range(len(data[phrase])-1): #Go through 0-(len-1) we always need to check i+1\n",
    "            transition[tags_dic[data[phrase][mot][1]],tags_dic[data[phrase][mot+1][1]]]+=1\n",
    "    transition = (transition/transition.sum(axis=1)[:,None])\n",
    "    return transition\n",
    "\n",
    "def calculate_observation_proba(data):\n",
    "    #compter P(X|Y), chaque colonne diviser par la somme colonne, presque comme dans TP1\n",
    "    #observation matrix\n",
    "    #Create a dic for words (word,nb of appearance)\n",
    "    association={}\n",
    "    for phrase in range(len(data[0:1500])):\n",
    "        for mot in range(len(data[phrase])):\n",
    "            if data[phrase][mot][0] not in association.keys():\n",
    "                association[data[phrase][mot][0]] = 1\n",
    "            else:\n",
    "                association[data[phrase][mot][0]] += 1\n",
    "    tags=[]\n",
    "    for phrase in range(len(data)):\n",
    "        for mot in range(len(data[phrase])):\n",
    "            if data[phrase][mot][1] not in tags:\n",
    "                tags.append(data[phrase][mot][1])\n",
    "    #For every word and label create the only entries to the obeservation matrix\n",
    "    words_dic={}\n",
    "    for i,j in zip(association.keys(),range(0,len(association.keys()))):\n",
    "        words_dic[i]=j\n",
    "    tags_dic={}\n",
    "    for i,j in zip(tags,range(0,len(tags))):\n",
    "        tags_dic[i]=j\n",
    "    #Init a zero matrix with size (len(words),len(tags))\n",
    "    observation = np.zeros([len(words_dic),len(tags)])\n",
    "    for phrase in range(len(data)):\n",
    "        for mot in range(len(data[phrase])):\n",
    "            #Thanks to the entries we created easy insertion it to the matrix\n",
    "            observation[words_dic[data[phrase][mot][0]],tags_dic[data[phrase][mot][1]]]+=1\n",
    "    for i in range(len(words_dic)):\n",
    "        #for every word, we calculate P(X|Y)\n",
    "        #words_dic.keys()[words_dic.values().index(i)] help us to find a key in a dict by a unique value\n",
    "        observation[i,:]=observation[i,:]/association[words_dic.keys()[words_dic.values().index(i)]]\n",
    "    return observation\n",
    "\n",
    "def calculate_initial_state(data):\n",
    "    initial_state_proba = np.zeros(len(etiquette), float ) \n",
    "    #juste compter l'état du premier mot, et divise par la somme\n",
    "    #initial_state\n",
    "    initial_state={}\n",
    "    for phrase in range(len(data)):\n",
    "        #From phrase we take first word's label\n",
    "            if data[phrase][0][1] not in initial_state.keys():\n",
    "                    initial_state[data[phrase][0][1]] = 1\n",
    "            else:\n",
    "                    initial_state[data[phrase][0][1]] += 1\n",
    "    count = 0\n",
    "    for key in initial_state.keys():\n",
    "        count+=initial_state[key]\n",
    "    count\n",
    "    for key in initial_state.keys():\n",
    "        initial_state[key] = initial_state[key]/count\n",
    "        \n",
    "    return np.array(initial_state.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09134984, 0.14114057, 0.12284618, 0.03667597, 0.21342867,\n",
       "       0.08892571, 0.15969655, 0.04513429, 0.00052319, 0.016812  ,\n",
       "       0.04912801, 0.03433903])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_initial_state(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 1. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       ...,\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 1. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0.5, 0. , ..., 0. , 0. , 0. ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_observation_proba(data)\n",
    "#Becareful this could take a while we have so many words P(X|Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.90506639e-03, 6.24966241e-01, 2.39750075e-01, 6.46783600e-02,\n",
       "        9.07292647e-03, 1.27809286e-02, 1.75181203e-02, 6.42331078e-04,\n",
       "        2.00728462e-03, 9.91233641e-03, 9.76635207e-03, 2.99997810e-03],\n",
       "       [1.55285163e-02, 1.49901215e-01, 1.29477207e-02, 1.59240196e-01,\n",
       "        2.45299525e-01, 2.84496271e-01, 2.64823172e-02, 5.98613369e-02,\n",
       "        1.78942457e-02, 1.98881655e-02, 8.09961580e-03, 3.60873971e-04],\n",
       "       [5.84299199e-03, 6.52336002e-01, 5.69363126e-02, 1.74811805e-02,\n",
       "        8.85051978e-02, 1.00418210e-01, 9.67857570e-03, 3.76030589e-02,\n",
       "        1.92854582e-02, 3.82363484e-03, 6.97813359e-03, 1.11124388e-03],\n",
       "       [1.63067759e-01, 9.74990145e-02, 5.75423766e-02, 1.84327231e-01,\n",
       "        1.69314747e-01, 8.06633525e-02, 1.03253252e-01, 1.43883317e-02,\n",
       "        6.56289694e-02, 5.50676711e-02, 9.00091980e-03, 2.46375542e-04],\n",
       "       [4.55657028e-01, 2.58417497e-01, 8.26275577e-02, 4.12205198e-02,\n",
       "        2.03028503e-02, 9.74730239e-03, 1.55017339e-02, 1.88590613e-03,\n",
       "        1.42444632e-02, 6.97854350e-02, 3.00985092e-02, 5.11197999e-04],\n",
       "       [1.09422858e-01, 1.33641813e-01, 4.66110965e-02, 1.23752954e-01,\n",
       "        1.05287915e-01, 1.72956594e-01, 7.03268574e-02, 1.12004463e-01,\n",
       "        2.95900061e-02, 7.48009101e-02, 1.94714273e-02, 2.13310580e-03],\n",
       "       [7.35694338e-02, 3.28002988e-02, 1.36359594e-01, 2.40452516e-01,\n",
       "        1.41927106e-01, 1.70227147e-01, 9.68711646e-02, 1.73251036e-02,\n",
       "        2.86913677e-02, 4.83644320e-02, 1.32873228e-02, 1.24513065e-04],\n",
       "       [1.51301476e-01, 2.43387769e-01, 1.11981965e-01, 1.95417966e-01,\n",
       "        7.32915673e-02, 2.08131275e-02, 9.13261160e-02, 2.62130069e-04,\n",
       "        2.51382736e-02, 6.76033448e-02, 1.86898739e-02, 7.86390207e-04],\n",
       "       [8.35680751e-02, 3.57142857e-02, 1.89134809e-02, 6.22669349e-01,\n",
       "        9.07444668e-02, 7.67270288e-02, 3.61837693e-02, 1.22065728e-02,\n",
       "        1.12340711e-02, 6.84104628e-03, 5.09725017e-03, 1.00603622e-04],\n",
       "       [1.75150520e-02, 8.85888625e-03, 9.48731983e-03, 7.06237710e-01,\n",
       "        5.58089562e-02, 1.03711813e-01, 5.40250157e-02, 1.13928926e-02,\n",
       "        2.37588437e-02, 8.18990857e-03, 9.93330495e-04, 2.02720509e-05],\n",
       "       [1.31810193e-02, 3.82519941e-01, 5.96187644e-02, 4.57617953e-02,\n",
       "        1.31810193e-01, 2.72204948e-01, 2.04812762e-02, 3.85291334e-02,\n",
       "        5.34000270e-03, 8.51696634e-03, 2.18331756e-02, 2.02784913e-04],\n",
       "       [1.00354191e-02, 5.31286895e-02, 2.95159386e-03, 8.85478158e-02,\n",
       "        6.19834711e-02, 3.03423849e-01, 1.00354191e-02, 3.36481700e-02,\n",
       "        9.44510035e-03, 7.67414404e-03, 5.90318772e-04, 4.18536009e-01]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_transition_proba(data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TC4-tp2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
